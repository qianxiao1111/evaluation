# -*- coding: utf-8 -*-
"""
@Time ： 2024/5/25 15:39
@Auth ： zhaliangyu
@File ：run_eval.py
@IDE ：PyCharm
"""
import re
import ast
import signal
import pandas as pd
import numpy as np
import json
import os
import datetime
from tqdm import tqdm
from typing import Optional, Any
from langchain_core.language_models import BaseLanguageModel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from evaluate_code_correction.utils import (
    filter_code,
    filter_cot,
    get_tool,
    extract_ori_observe,
)
from evaluate_code_correction.prompt import (
    RECTIFY_PROMPT_PYTHON_SYSTEM,
    RECTIFY_PROMPT_PYTHON_INSTRUCTION,
    CLASSIFY_PROMPT_PYTHON,
)

from contextlib import contextmanager

CODE_PREFIX = """import matplotlib.pyplot as plt
from mplfonts import use_font
import pandas as pd
import numpy as np
import seaborn as sns
import warnings

warnings.filterwarnings("ignore")
# Fixing Chinese font issues
use_font("Noto Serif CJK SC")\n"""


# 定义一个异常类，用于超时处理
class TimeoutException(Exception):
    pass


# 创建一个上下文管理器来处理超时
@contextmanager
def timeout(time):
    # 定义信号处理函数
    def raise_timeout(signum, frame):
        raise TimeoutException(f"Timeout error, running time exceed {time}")

    # 设置信号定时器
    signal.signal(signal.SIGALRM, raise_timeout)
    signal.alarm(time)
    try:
        yield
    finally:
        # 取消信号定时器
        signal.alarm(0)


def llm_eval(
    query: str,
    code: str,
    observation: str,
    true_result,
    llm: BaseLanguageModel,
) -> bool:
    """
    Compare the eval_llm output results and true_results by a Higher-level LLM, `gpt-4` etc, make this llm config in llms.py
    :param query: Human input query
    :param table_infos: Input table information
    :param code: Code generated by the llm_eval
    :param observation: Code execution result
    :param llm: llm_judge
    :return: Enum [True, False]
    """
    # print("True result", true_result)
    prompt = ChatPromptTemplate.from_messages([("system", CLASSIFY_PROMPT_PYTHON)])
    eval_chain = prompt | llm | StrOutputParser()
    # eval_chain.verbose = True
    input = {
        "query": query,
        # "table_infos": table_infos,
        "code": code,
        "observation": observation,
        "true_result": true_result,
    }
    output = eval_chain.invoke(input=input)
    res = output
    # print("Observe:", observation)
    # print("LLM eval results: ", res)
    return True if res.lower() == "yes" else False


def format_inputs(test_datas: list[dict], lan_type: str = "Python") -> list[list[dict]]:
    """
    Format inputs with prompts and input variances
    :param test_datas: loaded eval samples
    :param lan_type: Code type, support [`Python`] now
    :return
    """
    # 把需要推理的数据拼成 message 形式
    format_message_datas = []
    for idx, sample in tqdm(enumerate(test_datas)):
        queries = sample["query"]
        table_infos = sample["table_infos"]

        current_time = datetime.datetime.now().strftime("%Y-%m-%d:%H")
        output = sample["cot"] + f"{lan_type} Code:\n" + sample["code"]
        observes = sample["observation"]

        format_instruction = RECTIFY_PROMPT_PYTHON_INSTRUCTION.format(
            table_infos=table_infos,
            query=queries,
            observe=observes,
            current_time=current_time,
            output=output,
        )
        format_system = RECTIFY_PROMPT_PYTHON_SYSTEM
        messages = [
            {"role": "system", "content": format_system},
            {"role": "user", "content": format_instruction},
        ]
        format_message_datas.append(messages)

    return format_message_datas


def text_to_array(text: str) -> np.array:
    """
    将给定的文本转换为NumPy数组。
    支持数字、字符串以及简单结构（如元组、列表）的转换。

    参数:
    text (str): 输入的文本，可以是数字、字符串或结构化的数据表示。

    返回:
    numpy.ndarray: 转换后的NumPy数组。如果输入不支持转换，则返回None。
    """
    try:
        # 使用ast.literal_eval安全地解析文本
        parsed_data = ast.literal_eval(text)
        if isinstance(parsed_data, (int, float, str)):  # 如果是单一value
            return np.array([[parsed_data]])
        elif isinstance(parsed_data, (list, tuple)):  # 处理列表或元组
            try:
                return (
                    pd.DataFrame(parsed_data).dropna(how="all").drop_duplicates().values
                )
            except Exception:
                return None
        else:
            # print("警告：不支持 match 的数据类型或结构。")
            return None
    except (ValueError, SyntaxError) as e:
        # print(f"解析错误：{e}")
        return None


def compare_arrays(A: np.array, B: np.array, threshold: float = 0.5):
    """
    比较两个NumPy数组A和B。

    参数:
    - A, B: NumPy数组
    - threshold: 部分一致的阈值比例，默认为0.5

    返回:
    - tuple(bool, bool): 第一个布尔值表示是否存在完全一致的行或列，第二个布尔值表示是否存在部分一致（超过阈值）的行或列。
    """
    # 初始化比较结果
    full_match_found = False
    partial_match_found = False

    if A is not None and B is not None:
        # 转置B以方便同时比较行和列
        A_transposed = A.T
        B_transposed = B.T
        # 检查A的每一行是否与B的任一行或列完全一致
        for arr_A in [A, A_transposed]:
            for arr_B in [B, B_transposed]:
                for row_a in arr_A:
                    if row_a.shape[0] == arr_B.shape[1]:
                        if np.any(np.all(row_a == arr_B, axis=1)):
                            full_match_found = True
                    for row_b in arr_B:  # 这里的 row_b 可能是B的列
                        match_count = len(set(row_a) & set(row_b))
                        if match_count / len(row_b) >= threshold:  # a是预测的，b是答案
                            partial_match_found = True
    return full_match_found, partial_match_found


def eval_outputs(
    model_outputs: list[dict],
    eval_dataset_path: str,
    test_csv_file_path: str,
    lan_type: str,
) -> list[dict]:
    """
    Generate complete eval samples according to the eval_datasets
    and the model_outputs
    :param model_outputs: output_answers generate by the llm
    :param eval_dataset_path: eval dataset path
    :param test_csv_file_path: the csv files path
    :param
    :return Required complete output_answers List[Dict]
    """
    with open(eval_dataset_path, "r", encoding="utf-8") as f:
        test_datas = json.load(f)
    input_texts = [i["input_prompt"] for i in model_outputs]
    output_texts = [i["output_text"] for i in model_outputs]
    processed_data = []
    for idx, test_dt in enumerate(test_datas):
        llm_output = output_texts[idx]
        input_prompt = input_texts[idx]
        ori_error = extract_ori_observe(input_prompt)
        table_infos = test_datas[idx]["table_infos"]
        df_paths = test_datas[idx]["table_paths"]
        true_result = test_datas[idx]["true_result"]
        query = test_datas[idx]["query"]
        eval_result_sample = {}

        if len(df_paths) == 1:
            df = pd.read_csv(
                os.path.join(test_csv_file_path, df_paths[0]), low_memory=False
            )
        else:
            df = [
                pd.read_csv(os.path.join(test_csv_file_path, path), low_memory=False)
                for path in df_paths
            ]
        tool = get_tool(df)

        code, pure_code = filter_code(llm_output)
        cot = filter_cot(llm_output)
        # 运行超时代码，认为都是异常代码， 在tool.run()过程中，可能会print出额外的内容，不影响执行
        try:
            # 如果生成的代码为空（解析不到代码）， 也认为是llm没有理解observe内容或instruct， 输出为Code Error
            if not pure_code:
                observe = "Code Error: output empty code.."
            else:
                with timeout(5):  # 设置超时时间为15秒
                    pure_code = CODE_PREFIX + pure_code
                    observe = tool.run(pure_code)  # 需要监控超时的代码块

        except TimeoutException as e:
            observe = e
        except SystemExit as e:
            observe = f"SystemExit Error: {str(e)}"
        except Exception as e:
            observe = f"Unexpected Error: {str(e)}"

        eval_result_sample["code"] = CODE_PREFIX + code
        eval_result_sample["cot"] = cot
        eval_result_sample["observe"] = observe
        eval_result_sample["true_result"] = true_result
        eval_result_sample["table_infos"] = table_infos
        eval_result_sample["table_paths"] = df_paths
        eval_result_sample["query"] = query
        eval_result_sample["ori_error"] = ori_error
        processed_data.append(eval_result_sample)
    return processed_data


def execution_eval(observe: str, ori_error: str) -> bool:
    """
    Test whether the code generated by eval_llm can be executed.
    :param output: output code of llm generation
    :return: True or False
    """
    # 只要执行结果中不出现error 或者 exception， 就认为代码可执行
    pattern = re.compile(r"error|exception", re.IGNORECASE)

    try:
        res = not pattern.search(observe)
    except:
        res = True
    # print("Original Error:", ori_error)
    # print("Execute Observe:", observe)
    # print("Execute Result:", res)
    return res


def result_eval(observe: str, true_result: str) -> bool:
    """
    Whether observe equal to true_result
    :return: True or False
    """
    observe = observe.strip()
    while observe.endswith("\n"):
        observe = observe.strip("\n").strip()

    # 判断完全一致
    text_full_match = observe == true_result
    # 判断部分一致
    arr_pred = text_to_array(observe)
    arr_true = text_to_array(true_result)
    arr_full_match, arr_partial_match = compare_arrays(arr_pred, arr_true)
    return text_full_match, arr_full_match, arr_partial_match


def run_eval(
    eval_result_path: str = "../evalset/code_correction_test/results.json",
    llm_for_judge: Optional[BaseLanguageModel] = None,
):
    """
    Calculate eval pass rate, support execute_pass_rate and llm_eval_pass_rate
    :param eval_results_path:  Evaluation dataset path
    :param llm_for_judge: llm for classify the content generated by the llm_eval, this param is used while `eval_method == "execution",`
    :return: pass rate
    """
    # print(eval_answer)
    import json

    with open(eval_result_path, "r", encoding="utf-8") as f:
        samples = json.load(f)
    execute_passed, llm_eval_passed = 0, 0
    matched_all, matched_row, matched_row_patial = 0, 0, 0
    total_len = len(samples)
    for sample in tqdm(samples):
        code = sample["code"]
        observe = sample["observe"]
        ori_error = sample["ori_error"]
        true_result = sample["true_result"]
        query = sample["query"]
        execute_passed += 1 if execution_eval(observe, ori_error) else 0
        result = result_eval(observe, true_result)
        matched_all += 1 if result[0] else 0
        matched_row += 1 if result[1] else 0
        matched_row_patial += 1 if result[2] else 0
        if llm_for_judge is not None:
            llm_eval_passed += (
                1 if llm_eval(query, code, observe, true_result, llm_for_judge) else 0
            )
    print(f"Sample length: {total_len}. ")

    print(
        f"Execute Passed: {execute_passed}." f"\tExecute pass-rate is:",
        round(execute_passed / total_len, 3),
    )
    print(
        f"Exactly Matched: {matched_all}." f"\tResult accuracy is:",
        round(matched_all / total_len, 3),
    )
    print(
        f"Row/Col Matched: {matched_row}." f"\tResult accuracy is:",
        round(matched_row / total_len, 3),
    )
    print(
        f"Row/Col Partial: {matched_row_patial}." f"\tResult accuracy is:",
        round(matched_row_patial / total_len, 3),
    )

    if llm_for_judge is not None:
        print(f"LLM eval Passed: {llm_eval_passed}")
        print(f"LLM_eval pass-rate is:", round(llm_eval_passed / total_len, 3))
